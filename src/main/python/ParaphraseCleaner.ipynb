{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-2-21\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "import pymysql\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import random\n",
    "from langdetect import detect\n",
    "import csv\n",
    "import datetime\n",
    "import MySQLdb\n",
    "import pandas as pd\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "today = str(now.year) + \"-\" + str(now.month) + \"-\" + str(now.day)\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def execute_sql(conn, query):\n",
    "    cur = conn.cursor(MySQLdb.cursors.DictCursor)\n",
    "    cur.execute(query)\n",
    "    return cur\n",
    "\n",
    "conn = MySQLdb.connect(host='127.0.0.1', user='root', passwd=\"\", db='vroniplag', charset='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_query_monolingual_table():\n",
    "    query = \"CREATE TABLE monolingual AS SELECT annotation_identifier, url, \"\n",
    "    query += \"plagiat_sent, source_sent, fake_source_sent, lang_plagiat, lang_source, \"\n",
    "    query += \"bow_diff, nb_words_ratio \"\n",
    "    query += \"from annotationAugmented \"\n",
    "    query += \"WHERE category='Verschleierung' and lang_plagiat=lang_source\"\n",
    "    #query += \"nb_words_ratio>\" + str(word_ratio_min) + \" and nb_words_ratio<\" + str(word_ratio_max) + \" and \"\n",
    "    #query += \"length(plagiat_sent)>\" + str(len_min) + \" and length(source_sent)>\" + str(len_min) + \" and \"\n",
    "    #query += \"bow_diff>\" + str(bow_diff_min)\n",
    "    query += \";\"\n",
    "    print(query)\n",
    "    return query\n",
    "\n",
    "def get_query_monolingual_lang_table(tablename, lang, word_ratio_min, word_ratio_max, bow_diff_min=1):\n",
    "    query = \"CREATE TABLE \" + tablename + \" as SELECT annotation_identifier, url, plagiat_sent, source_sent, \"\n",
    "    query += \"bow_diff, nb_words_ratio \"\n",
    "    query += \"from monolingual where lang_plagiat='\" + lang + \"' and lang_source='\" + lang + \"' and \"\n",
    "    query += \"nb_words_ratio>\" + str(word_ratio_min) + \" and nb_words_ratio<\" + str(word_ratio_max) + \" and \"\n",
    "    #query += \"length(plagiat_sent)>\" + str(len_min) + \" and length(source_sent)>\" + str(len_min) + \" and \"\n",
    "    query += \"bow_diff>\" + str(bow_diff_min)\n",
    "    query += \";\"\n",
    "    print(query)\n",
    "    return query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data_from_monolingual_lang_table(lang, keys, isParaphrase, excludeInnerIdenticals=True):\n",
    "    query = \"SELECT * from monolingual\" + lang.upper() + \" WHERE isParaphrase=\" + str(isParaphrase) + \";\"\n",
    "    cur = execute_sql(conn, query)\n",
    "    rows = []\n",
    "    counter = 0\n",
    "    for row in cur:\n",
    "        #if row['annotation_identifier'] != \"Aaf/Fragment 009 01_7\":\n",
    "        #    continue\n",
    "            \n",
    "        new_row = []\n",
    "        for key in keys:\n",
    "            val = row[key]\n",
    "            #print(type(val))\n",
    "            if type(val)==unicode:\n",
    "                #print(val)\n",
    "                #val = val.decode('latin-1').encode(\"utf-8\")\n",
    "                val = val.encode(\"utf-8\")\n",
    "                #print(val)\n",
    "                #print(\"\")\n",
    "                \n",
    "            new_row.append(val)\n",
    "        if excludeInnerIdenticals:\n",
    "            plag = row['plagiat_sent'].lower()\n",
    "            src = row['source_sent'].lower()\n",
    "            if plag in src or src in plag:\n",
    "                counter += 1\n",
    "                continue    \n",
    "        rows.append(new_row)\n",
    "    \n",
    "    print(\"# skipped (inner identicals): \" + str(counter))\n",
    "    return rows\n",
    "\n",
    "def split_data(percent_test, data):\n",
    "    split = int(percent_test * len(data))\n",
    "    test_data = data[:split+1]\n",
    "    train_data = data[split+1:]\n",
    "    return train_data, test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Include negative paraphrase pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addValToQuery(vals, query):\n",
    "    for idx, key in enumerate(vals):\n",
    "        if idx==len(vals)-1:\n",
    "            query += str(key) + \") \"\n",
    "        else:\n",
    "            query += str(key) + \", \"\n",
    "    return query\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addNegatives(conn, tablename, fakeSrcDict):\n",
    "    err_count = 0\n",
    "    nb_negatives = 0\n",
    "    cur = execute_sql(conn, \"SELECT * from \" + tablename + \" WHERE isParaphrase=1;\")\n",
    "    for row in cur:\n",
    "        annotation_id = row['annotation_identifier']\n",
    "        fake_src = fakeSrcDict[annotation_id]\n",
    "        if not fake_src: continue\n",
    "        row['annotation_identifier'] = annotation_id + \"_f\"\n",
    "        row['source_sent'] = fake_src\n",
    "        row['isParaphrase'] = 0\n",
    "        \n",
    "        query = \"INSERT INTO \" + tablename + \" (\"\n",
    "        query = addValToQuery(row.keys(), query)\n",
    "        query += \" VALUES (\"\n",
    "        values = []\n",
    "        for item in row.values():\n",
    "            if type(item) == unicode:\n",
    "                item = item.replace(\"'\",\"\\\\'\")\n",
    "                item = item.encode('utf-8')\n",
    "            try:\n",
    "                values.append(\"'\"+str(item)+\"'\")\n",
    "            except Exception as e: \n",
    "                err_count += 1\n",
    "                # print(item)\n",
    "                \n",
    "        query = addValToQuery(values, query)\n",
    "        query += \";\"\n",
    "        try:\n",
    "            cur2 = conn.cursor()\n",
    "            cur2.execute(query)\n",
    "            cur2.close()\n",
    "            nb_negatives += 1\n",
    "        except Exception as e:\n",
    "            ## Fehler passieren durch Abostrophe im Text ... \n",
    "            print(row['url'])\n",
    "            #if row['url'] == 'http://de.vroniplag.wikia.com/wiki/Ww/Fragment_016_01' or row['url'] == 'http://de.vroniplag.wikia.com/wiki/Yb/Fragment_186_14':\n",
    "                #print(query)\n",
    "    print(\"#encoding errors: {}\".format(err_count))    \n",
    "    print(\"#nb fake pairs: {}\".format(nb_negatives))\n",
    "    \n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fake Sources**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get mapping from annotation_identifier to fake source sent\n",
    "def getFakeSources(lang):\n",
    "    query = \"SELECT * from monolingual WHERE lang_plagiat='\"+lang+\"' and lang_source='\"+lang+\"';\"\n",
    "    cur_annotation = execute_sql(conn, query)\n",
    "    ids_to_fake_sources = {}\n",
    "    for row in cur_annotation:\n",
    "        key = row['annotation_identifier']\n",
    "        ids_to_fake_sources[key] = row['fake_source_sent']\n",
    "\n",
    "    cur_annotation.close()\n",
    "    return ids_to_fake_sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate CSV-Files in UTF-8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keys = ['plagiat_sent', 'source_sent', 'isParaphrase', 'url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# skipped (inner identicals): 0\n"
     ]
    }
   ],
   "source": [
    "get_data_from_monolingual_lang_table('es', keys, 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_csv_files(lang, suffix, keys):\n",
    "    data_isPP = get_data_from_monolingual_lang_table(lang, keys, 1)\n",
    "    print(\"nb paraphrases: \" + str(len(data_isPP)))\n",
    "    data_noPP = get_data_from_monolingual_lang_table(lang, keys, 0)\n",
    "    print(\"nb fake-paraphrases: \" + str(len(data_noPP)))\n",
    "\n",
    "    train_data_isPP, test_data_isPP = split_data(0.2, data_isPP)\n",
    "    train_data_noPP, test_data_noPP = split_data(0.2, data_noPP)\n",
    "\n",
    "    train_data = pd.DataFrame(train_data_isPP + train_data_noPP,columns=keys)\n",
    "    test_data = pd.DataFrame(test_data_isPP + test_data_noPP, columns=keys)\n",
    "\n",
    "    print(\"train shape: \" + str(train_data.shape))\n",
    "    print(\"test shape: \" + str(test_data.shape))\n",
    "    train_data.to_csv(lang+ suffix +'_train.csv', encoding='utf-8')\n",
    "    test_data.to_csv(lang+ suffix + '_test.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enrich the *annotation*-table with the columns **category**, **lang_source** and **lang_plagiat** from the *fragment*-table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"CREATE TABLE annotationAugmented AS SELECT * from annotation\"\n",
    "cur = execute_sql(conn, query)\n",
    "conn.commit()\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALTER TABLE annotationAugmented ADD COLUMN lang_source VARCHAR(80), ADD COLUMN lang_plagiat VARCHAR(80), ADD COLUMN category VARCHAR(80);\n"
     ]
    }
   ],
   "source": [
    "query = \"ALTER TABLE annotationAugmented ADD COLUMN lang_source VARCHAR(80), ADD COLUMN lang_plagiat VARCHAR(80), \"\n",
    "query += \"ADD COLUMN category VARCHAR(80);\"\n",
    "print(query)\n",
    "cur = execute_sql(conn, query)\n",
    "conn.commit()\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lang_source'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-019381c68599>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcur_fragment\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'url'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mlang_original\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lang_source'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mlang_plagiat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lang_plagiat'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mcategory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'category'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'lang_source'"
     ]
    }
   ],
   "source": [
    "# Insert data\n",
    "cur_fragment = execute_sql(conn, \"SELECT * from fragment\")\n",
    "count=0\n",
    "for row in cur_fragment:\n",
    "    url = row['url']\n",
    "    lang_original = row['lang_source']\n",
    "    lang_plagiat = row['lang_plagiat']\n",
    "    category = row['category']\n",
    "    query = \"UPDATE annotationAugmented SET lang_source='\" + lang_original + \"', lang_plagiat='\" + lang_plagiat;\n",
    "    query += \"', category='\" + category;\n",
    "    query += \"' WHERE url='\" + url + \"';\"\n",
    "    execute_sql(conn, query)\n",
    "    if(count % 1000 == 0):\n",
    "        print(\"{0} processed\".format(count))\n",
    "    count+=1\n",
    "    \n",
    "conn.commit()\n",
    "cur_fragment.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Monolingual database from *annotationAugmented*-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE monolingual AS SELECT annotation_identifier, url, plagiat_sent, source_sent, fake_source_sent, lang_plagiat, lang_source, bow_diff, nb_words_ratio from annotationAugmented WHERE category='Verschleierung' and lang_plagiat=lang_source;\n"
     ]
    }
   ],
   "source": [
    "query = get_query_monolingual_table()\n",
    "cur = execute_sql(conn, query)\n",
    "conn.commit()\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MonolingualEN**-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE monolingualEN as SELECT annotation_identifier, url, plagiat_sent, source_sent, bow_diff, nb_words_ratio from monolingual where lang_plagiat='en' and lang_source='en' and nb_words_ratio>0.5 and nb_words_ratio<1.5 and bow_diff>6;\n"
     ]
    }
   ],
   "source": [
    "query = get_query_monolingual_lang_table(\"monolingualEN\", \"en\", word_ratio_min=0.5, word_ratio_max=1.5,\n",
    "                                         bow_diff_min=6)\n",
    "cur = execute_sql(conn, query)\n",
    "conn.commit()\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"ALTER TABLE monolingualEN ADD COLUMN isParaphrase TINYINT(1) DEFAULT 1\"\n",
    "cur = execute_sql(conn, query)\n",
    "conn.commit()\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#encoding errors: 0\n",
      "#nb fake pairs: 1101\n"
     ]
    }
   ],
   "source": [
    "ids_to_fake_sources = getFakeSources(\"en\")\n",
    "addNegatives(conn, \"monolingualEN\", ids_to_fake_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# skipped (inner identicals): 7\n",
      "nb paraphrases: 1183\n",
      "# skipped (inner identicals): 0\n",
      "nb fake-paraphrases: 1101\n",
      "train shape: (1826, 4)\n",
      "test shape: (458, 4)\n"
     ]
    }
   ],
   "source": [
    "# write train and test csv files\n",
    "write_csv_files(\"en\",today, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MonolingualDE**-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE monolingualDE as SELECT annotation_identifier, url, plagiat_sent, source_sent, bow_diff, nb_words_ratio from monolingual where lang_plagiat='de' and lang_source='de' and nb_words_ratio>0.5 and nb_words_ratio<1.5 and bow_diff>6;\n"
     ]
    }
   ],
   "source": [
    "query = get_query_monolingual_lang_table(\"monolingualDE\", \"de\", word_ratio_min=0.5, word_ratio_max=1.5,\n",
    "                                         bow_diff_min=6)\n",
    "cur = execute_sql(conn, query)\n",
    "conn.commit()\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"ALTER TABLE monolingualDE ADD COLUMN isParaphrase TINYINT(1) DEFAULT 1\"\n",
    "cur = execute_sql(conn, query)\n",
    "conn.commit()\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#encoding errors: 0\n",
      "#nb fake pairs: 8556\n"
     ]
    }
   ],
   "source": [
    "ids_to_fake_sources = getFakeSources(\"de\")\n",
    "addNegatives(conn, \"monolingualDE\", ids_to_fake_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# skipped (inner identicals): 29\n",
      "nb paraphrases: 9501\n",
      "# skipped (inner identicals): 1\n",
      "nb fake-paraphrases: 8555\n",
      "train shape: (14443, 4)\n",
      "test shape: (3613, 4)\n"
     ]
    }
   ],
   "source": [
    "# write train and test csv files\n",
    "write_csv_files(\"de\", today, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MonolingualES**-table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE monolingualES as SELECT annotation_identifier, url, plagiat_sent, source_sent, bow_diff, nb_words_ratio from monolingual where lang_plagiat='es' and lang_source='es' and nb_words_ratio>0.5 and nb_words_ratio<1.5 and bow_diff>6;\n"
     ]
    }
   ],
   "source": [
    "query = get_query_monolingual_lang_table(\"monolingualES\", \"es\", word_ratio_min=0.5, word_ratio_max=1.5,\n",
    "                                         bow_diff_min=6)\n",
    "cur = execute_sql(conn, query)\n",
    "conn.commit()\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = \"ALTER TABLE monolingualES ADD COLUMN isParaphrase TINYINT(1) DEFAULT 1\"\n",
    "cur = execute_sql(conn, query)\n",
    "conn.commit()\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#encoding errors: 0\n",
      "#nb fake pairs: 2\n"
     ]
    }
   ],
   "source": [
    "###Einbauen, dass jedes annotation_identifier nur einmal eingef√ºgt werden darf!! D.h. jedes negative Beispiel darf\n",
    "### nur einmal eingebaut werden!!\n",
    "# MonolingualES\n",
    "ids_to_fake_sources = getFakeSources(\"es\")\n",
    "addNegatives(conn, \"monolingualES\", ids_to_fake_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# skipped (inner identicals): 0\n",
      "nb paraphrases: 2\n",
      "# skipped (inner identicals): 0\n",
      "nb fake-paraphrases: 2\n",
      "train shape: (2, 4)\n",
      "test shape: (2, 4)\n"
     ]
    }
   ],
   "source": [
    "# write train and test csv files\n",
    "write_csv_files(\"es\", today, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
